# macros to include
load = ndlMnistMacros

# the actual NDL that defines the network
run = DNN

ndlMnistMacros = [
    imageW = 19
    imageH = 19
	imageC = 52
    labelDim = 1

    features = ImageInput(imageW, imageH, imageC, tag = feature, imageLayout=$imageLayout$)
    win = Input(1, tag = label)
    move = Input(361, tag = label)
    statistic = ImageInput(imageW, imageH, 1, tag = label, imageLayout=$imageLayout$)

    scValue = 1
    expAvg = 1024
    
    convWScale = 1
    convBValue = 0
]

DNN = [
# conv1
    kW1 = 5
    kH1 = 5
    # cMap1 = 192
    # cMap1x9 = 1728
    # cMap2 = 128
    # cMap2x9 = 1152
    # cMap3 = 128 #192
    # cMap3x9 = 1152 #1728

    cMap1 = 192
    cMap1x9 = 1728
    cMap2 = 128
    cMap2x9 = 1152
    cMap3 = 64
    cMap3x9 = 576

	ones = Constant(1, rows=1, cols=361)

    stride1 = 1
    # weight[cMap1, kW1 * kH1 * inputChannels]
	conv1 = ConvBNReLULayer(features, cMap1, 1300, kW1, kH1, stride1, stride1, convWScale, convBValue, scValue, expAvg)
	rn1_1  = ResNetNode2(conv1, cMap1, cMap1, cMap1x9, convWScale, convBValue, scValue, expAvg)
	rn1_2  = ResNetNode2(rn1_1, cMap1, cMap1, cMap1x9, convWScale, convBValue, scValue, expAvg)
	rn1    = ResNetNode2(rn1_2, cMap1, cMap1, cMap1x9, convWScale, convBValue, scValue, expAvg)

	# conv2  = ConvBNReLULayer(rn1, cMap2, cMap1, 1, 1, stride1, stride1, convWScale, convBValue, scValue, expAvg)
	# rn2_1  = ResNetNode2(conv2, cMap2, cMap2, cMap2x9, convWScale, convBValue, scValue, expAvg)
	# rn2_2  = ResNetNode2(rn2_1, cMap2, cMap2, cMap2x9, convWScale, convBValue, scValue, expAvg)
	# # rn2_3  = ResNetNode2(rn2_2, cMap2, cMap2, cMap2x9, convWScale, convBValue, scValue, expAvg)
	# rn2    = ResNetNode2(rn2_2, cMap2, cMap2, cMap2x9, convWScale, convBValue, scValue, expAvg)

	## Move precision #################
    kWL = 3
    kHL = 3
	rnm_conv  = ConvBNReLULayer(rn1, cMap3, cMap1, 1, 1, stride1, stride1, convWScale, convBValue, scValue, expAvg)
	rnm_1   = ResNetNode2(rnm_conv, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnm_2   = ResNetNode2(rnm_1, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnm_3   = ResNetNode2(rnm_2, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnm     = ResNetNode2(rnm_3, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	conv_move = ConvLayer(rnm, 1, cMap3x9, kWL, kHL, stride1, stride1, convWScale, convBValue)
	ol = Reshape(conv_move, 361, numChannels=361, imageWidth=1, imageHeight=1)

	ce_move = CrossEntropyWithSoftmax(move, ol)
    err_move = ErrorPrediction(move, ol)
	###################################

	## Value ##########################
	rnv_conv  = ConvBNReLULayer(rn1, cMap3, cMap1, 1, 1, stride1, stride1, convWScale, convBValue, scValue, expAvg)
	rnv_1   = ResNetNode2(rnv_conv, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnv_2   = ResNetNode2(rnv_1, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnv_3   = ResNetNode2(rnv_2, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	rnv     = ResNetNode2(rnv_3, cMap3, cMap3, cMap3x9, convWScale, convBValue, scValue, expAvg)
	conv_owner = ConvLayer2(rnv, 1, cMap3, 19, 19, 1, 1, stride1, stride1, convWScale, convBValue)
	conv_value = ConvLayer2(rnv, 1, cMap3, 19, 19, 1, 1, stride1, stride1, convWScale, convBValue)

	# ones = Parameter(1, 361, init = fixedValue, value = 1, learningRateMultiplier = 0)
	sum = Times(ones, Tanh(conv_value))
	#sum = Times(ones, conv_value)
	#conv_value_t = RectifiedLinear(conv_value)
	#sum = DNNLayer(conv_value_t, 361, 1, 1)
    # ofsV = Parameter(1, 1, init="uniform", initValue=-6.5)
    # scaleV = Parameter(1, 1, init="uniform", initValue=0.12)
	# full = Times(Plus(sum, ofsV), scaleV)
	# p = Tanh(full)
	full = Times(sum, Constant(0.01))
	#p = Tanh(sum)
	p = Tanh(full)

	owner = Sigmoid(conv_owner)
	#owner = Sigmoid(conv_value)
	onwers_diff = Minus(owner, statistic)
	owners2 = ElementTimes(onwers_diff, onwers_diff)

	err_value = SquareError(win, p)
	#err_value = Abs(Minus(win,  p))
	err_owner = Times(ones, owners2)
	ce_value = err_value
	ce_owner = err_owner
	# ce_owner_m = err_owner_m
	#############

	#ce = Plus(Plus(ce_move, ce_owner), err_value)
	ce = Plus(ce_move, ce_owner)
	#ce = Plus(ce_move, Plus(ce_owner, ce_owner_m))
	#ce_1 = Plus(Times(ce_owner, Constant(0.01)), Plus(ce_value, ce_move))
	#ce_1 = Plus(ce_owner, Plus(ce_value, ce_move))
	ce_1 = Plus(ce_owner, Plus(ce_move, ce_value))
	ce_2 = Plus(ce_move, ce_value)
	ce_3 = Plus(ce_value, Times(Constant(0.01), ce_owner))

    # ce = CrossEntropyWithSoftmax(labels, ol)
    # err = ErrorPrediction(labels, ol)

    # Special Nodes
    FeatureNodes = (features)
    LabelNodes = (win, move)
	#CriterionNodes = (ce_move, ce_value, ce_owner)
    CriterionNodes = (ce)
    #CriterionNodes = (err_value)
    EvalNodes = (err_move, err_value, err_owner, ce, ce_1, ce_2, ce_3)
    OutputNodes = (p, owner)
]
